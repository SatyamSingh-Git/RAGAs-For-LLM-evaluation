{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAs Integration with LLM Log JSON\n",
    "\n",
    "This notebook integrates RAGAs metrics into LLM log evaluation using Google's Gemini 2.5 Flash model.\n",
    "\n",
    "## Metrics Computed:\n",
    "- **Faithfulness**: Factual consistency with context\n",
    "- **Answer Relevancy**: Relevance to user query\n",
    "- **Context Precision**: Proportion of relevant context chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ragas in e:\\new folder\\.venv\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: langchain-google-genai in e:\\new folder\\.venv\\lib\\site-packages (2.1.8)\n",
      "Requirement already satisfied: langchain-core in e:\\new folder\\.venv\\lib\\site-packages (0.3.72)\n",
      "Requirement already satisfied: numpy in e:\\new folder\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: datasets in e:\\new folder\\.venv\\lib\\site-packages (from ragas) (4.0.0)\n",
      "Requirement already satisfied: tiktoken in e:\\new folder\\.venv\\lib\\site-packages (from ragas) (0.9.0)\n",
      "Requirement already satisfied: langchain in e:\\new folder\\.venv\\lib\\site-packages (from ragas) (0.3.27)\n",
      "Requirement already satisfied: langchain-community in e:\\new folder\\.venv\\lib\\site-packages (from ragas) (0.3.27)\n",
      "Requirement already satisfied: langchain_openai in e:\\new folder\\.venv\\lib\\site-packages (from ragas) (0.3.28)\n",
      "Requirement already satisfied: nest-asyncio in e:\\new folder\\.venv\\lib\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in e:\\new folder\\.venv\\lib\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in e:\\new folder\\.venv\\lib\\site-packages (from ragas) (2.11.7)\n",
      "Requirement already satisfied: openai>1 in e:\\new folder\\.venv\\lib\\site-packages (from ragas) (1.97.1)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in e:\\new folder\\.venv\\lib\\site-packages (from ragas) (5.6.3)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-google-genai) (0.6.18)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-core) (0.4.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-core) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in e:\\new folder\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in e:\\new folder\\.venv\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.40.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in e:\\new folder\\.venv\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in e:\\new folder\\.venv\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (6.31.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in e:\\new folder\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\new folder\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in e:\\new folder\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (3.11.1)\n",
      "Requirement already satisfied: requests<3,>=2 in e:\\new folder\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in e:\\new folder\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in e:\\new folder\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in e:\\new folder\\.venv\\lib\\site-packages (from openai>1->ragas) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\new folder\\.venv\\lib\\site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in e:\\new folder\\.venv\\lib\\site-packages (from openai>1->ragas) (0.10.0)\n",
      "Requirement already satisfied: sniffio in e:\\new folder\\.venv\\lib\\site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in e:\\new folder\\.venv\\lib\\site-packages (from openai>1->ragas) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\new folder\\.venv\\lib\\site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\new folder\\.venv\\lib\\site-packages (from pydantic>=2->ragas) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\new folder\\.venv\\lib\\site-packages (from pydantic>=2->ragas) (0.4.1)\n",
      "Requirement already satisfied: filelock in e:\\new folder\\.venv\\lib\\site-packages (from datasets->ragas) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in e:\\new folder\\.venv\\lib\\site-packages (from datasets->ragas) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\new folder\\.venv\\lib\\site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in e:\\new folder\\.venv\\lib\\site-packages (from datasets->ragas) (2.3.1)\n",
      "Requirement already satisfied: xxhash in e:\\new folder\\.venv\\lib\\site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in e:\\new folder\\.venv\\lib\\site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in e:\\new folder\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in e:\\new folder\\.venv\\lib\\site-packages (from datasets->ragas) (0.34.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in e:\\new folder\\.venv\\lib\\site-packages (from langchain->ragas) (0.3.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in e:\\new folder\\.venv\\lib\\site-packages (from langchain->ragas) (2.0.41)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-community->ragas) (3.12.14)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-community->ragas) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in e:\\new folder\\.venv\\lib\\site-packages (from langchain-community->ragas) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in e:\\new folder\\.venv\\lib\\site-packages (from tiktoken->ragas) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\new folder\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->ragas) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in e:\\new folder\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->ragas) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\new folder\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->ragas) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\new folder\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->ragas) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\new folder\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->ragas) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\new folder\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->ragas) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\new folder\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->ragas) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\new folder\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in e:\\new folder\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in e:\\new folder\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in e:\\new folder\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in e:\\new folder\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in e:\\new folder\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in e:\\new folder\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\new folder\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\new folder\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in e:\\new folder\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\new folder\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\new folder\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in e:\\new folder\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\new folder\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\new folder\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in e:\\new folder\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.2.3)\n",
      "Requirement already satisfied: colorama in e:\\new folder\\.venv\\lib\\site-packages (from tqdm>4->openai>1->ragas) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\new folder\\.venv\\lib\\site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\new folder\\.venv\\lib\\site-packages (from pandas->datasets->ragas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\new folder\\.venv\\lib\\site-packages (from pandas->datasets->ragas) (2025.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in e:\\new folder\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\new folder\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in e:\\new folder\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ragas langchain-google-genai langchain-core numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in e:\\new folder\\.venv\\lib\\site-packages (11.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\New folder\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "# RAGAs imports\n",
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy, LLMContextPrecisionWithoutReference\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# LangChain Google GenAI imports\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Google API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Google API key is set\n"
     ]
    }
   ],
   "source": [
    "# Set your Google API key here\n",
    "# Get your API key from: https://ai.google.dev/\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAkogaBOk8_x-45UPqLR1f5kykEVtH9nIE\"\n",
    "\n",
    "# Verify the API key is set\n",
    "if not os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\") == \"your-google-api-key-here\":\n",
    "    print(\"‚ö†Ô∏è  Please set your actual Google API key above\")\n",
    "else:\n",
    "    print(\"‚úÖ Google API key is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAGAs Evaluator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGAsEvaluator:\n",
    "    \"\"\"\n",
    "    RAGAs evaluator for LLM logs using Gemini models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, google_api_key: str = None):\n",
    "        \"\"\"Initialize the RAGAs evaluator with Gemini models.\"\"\"\n",
    "        if google_api_key:\n",
    "            os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
    "        elif not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "            raise ValueError(\"Google API key must be provided\")\n",
    "        \n",
    "        # Initialize Gemini 1.5 Flash model (more stable for free tier)\n",
    "        self.llm = LangchainLLMWrapper(\n",
    "            ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                temperature=0.1,\n",
    "                max_tokens=1024,\n",
    "                timeout=100  # Changed from request_timeout to timeout\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Initialize Google embeddings\n",
    "        self.embeddings = LangchainEmbeddingsWrapper(\n",
    "            GoogleGenerativeAIEmbeddings(\n",
    "                model=\"models/embedding-001\",\n",
    "                task_type=\"retrieval_document\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Initialize RAGAs metrics\n",
    "        self.faithfulness_metric = Faithfulness(llm=self.llm)\n",
    "        self.answer_relevancy_metric = ResponseRelevancy(\n",
    "            llm=self.llm, \n",
    "            embeddings=self.embeddings\n",
    "        )\n",
    "        self.context_precision_metric = LLMContextPrecisionWithoutReference(\n",
    "            llm=self.llm\n",
    "        )\n",
    "    \n",
    "    def load_logs(self, log_file_path: str):\n",
    "        \"\"\"Load log data from JSON file.\"\"\"\n",
    "        with open(log_file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    def extract_evaluation_data(self, log_data) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract evaluation data from log structure.\"\"\"\n",
    "        evaluation_samples = []\n",
    "        \n",
    "        # Handle list structure (your actual JSON format)\n",
    "        if isinstance(log_data, list):\n",
    "            logs = log_data\n",
    "        else:\n",
    "            logs = log_data.get(\"logs\", [])\n",
    "        \n",
    "        for log_entry in logs:\n",
    "            items = log_entry.get(\"items\", [])\n",
    "            \n",
    "            for item in items:\n",
    "                item_id = item.get(\"id\", f\"item-{len(evaluation_samples) + 1}\")\n",
    "                input_data_list = item.get(\"input\", [])\n",
    "                expected_output_list = item.get(\"expectedOutput\", [])\n",
    "                \n",
    "                # Extract system and user content from input list\n",
    "                system_prompt = \"\"\n",
    "                user_query = \"\"\n",
    "                \n",
    "                for input_item in input_data_list:\n",
    "                    if isinstance(input_item, dict):\n",
    "                        role = input_item.get(\"role\", \"\")\n",
    "                        if role == \"system\":\n",
    "                            system_prompt = input_item.get(\"context\", \"\")\n",
    "                        elif role == \"user\":\n",
    "                            user_query = input_item.get(\"context\", \"\")\n",
    "                \n",
    "                # Extract expected output (response) from expectedOutput list\n",
    "                expected_output = \"\"\n",
    "                for output_item in expected_output_list:\n",
    "                    if isinstance(output_item, dict) and output_item.get(\"role\") == \"assistant\":\n",
    "                        expected_output = output_item.get(\"content\", \"\")\n",
    "                        break\n",
    "                \n",
    "                # Only add if we have both query and expected output\n",
    "                if user_query and expected_output:\n",
    "                    evaluation_samples.append({\n",
    "                        \"id\": item_id,\n",
    "                        \"query\": user_query,\n",
    "                        \"context\": system_prompt,\n",
    "                        \"response\": expected_output\n",
    "                    })\n",
    "        \n",
    "        return evaluation_samples\n",
    "    \n",
    "    async def compute_ragas_metrics(self, sample_data: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Compute RAGAs metrics for a single sample.\"\"\"\n",
    "        try:\n",
    "            sample = SingleTurnSample(\n",
    "                user_input=sample_data[\"query\"],\n",
    "                response=sample_data[\"response\"],\n",
    "                retrieved_contexts=[sample_data[\"context\"]] if sample_data[\"context\"] else []\n",
    "            )\n",
    "            \n",
    "            # Add delay to avoid rate limiting\n",
    "            await asyncio.sleep(25)  # Increased delay to 15 seconds\n",
    "            \n",
    "            # Compute metrics with error handling\n",
    "            try:\n",
    "                faithfulness_score = await self.faithfulness_metric.single_turn_ascore(sample)\n",
    "            except Exception as e:\n",
    "                print(f\"Faithfulness error: {e}\")\n",
    "                faithfulness_score = 0.0\n",
    "            \n",
    "            await asyncio.sleep(25)  # Another delay\n",
    "            \n",
    "            try:\n",
    "                answer_relevancy_score = await self.answer_relevancy_metric.single_turn_ascore(sample)\n",
    "            except Exception as e:\n",
    "                print(f\"Answer relevancy error: {e}\")\n",
    "                answer_relevancy_score = 0.0\n",
    "            \n",
    "            await asyncio.sleep(25)  # Another delay\n",
    "            \n",
    "            try:\n",
    "                if sample_data[\"context\"]:\n",
    "                    context_precision_score = await self.context_precision_metric.single_turn_ascore(sample)\n",
    "                else:\n",
    "                    context_precision_score = 0.0\n",
    "            except Exception as e:\n",
    "                print(f\"Context precision error: {e}\")\n",
    "                context_precision_score = 0.0\n",
    "            \n",
    "            return {\n",
    "                \"faithfulness\": float(faithfulness_score) if not np.isnan(faithfulness_score) else 0.0,\n",
    "                \"answer_relevancy\": float(answer_relevancy_score) if not np.isnan(answer_relevancy_score) else 0.0,\n",
    "                \"context_precision\": float(context_precision_score) if not np.isnan(context_precision_score) else 0.0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error computing metrics: {e}\")\n",
    "            return {\"faithfulness\": 0.0, \"answer_relevancy\": 0.0, \"context_precision\": 0.0}\n",
    "    \n",
    "    async def evaluate_logs(self, log_file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Evaluate all samples in the log file.\"\"\"\n",
    "        log_data = self.load_logs(log_file_path)\n",
    "        evaluation_samples = self.extract_evaluation_data(log_data)\n",
    "        \n",
    "        if not evaluation_samples:\n",
    "            print(\"‚ö†Ô∏è No evaluation samples found. Check your JSON structure.\")\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, sample in enumerate(evaluation_samples):\n",
    "            print(f\"Evaluating {i+1}/{len(evaluation_samples)}: {sample['id']}\")\n",
    "            \n",
    "            metrics = await self.compute_ragas_metrics(sample)\n",
    "            \n",
    "            result = {\n",
    "                \"id\": sample[\"id\"],\n",
    "                \"faithfulness\": round(metrics[\"faithfulness\"], 4),\n",
    "                \"answer_relevancy\": round(metrics[\"answer_relevancy\"], 4),\n",
    "                \"context_precision\": round(metrics[\"context_precision\"], 4)\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            print(f\"  Faithfulness: {result['faithfulness']}, Relevancy: {result['answer_relevancy']}, Precision: {result['context_precision']}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_results(self, results: List[Dict[str, Any]], output_file: str = \"ragas_evaluation_results.json\"):\n",
    "        \"\"\"Save evaluation results to JSON file.\"\"\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Preview Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of log_data: <class 'list'>\n",
      "Number of log entries: 10\n",
      "\n",
      "First log entry structure:\n",
      "{\n",
      "  \"metaData\": {\n",
      "    \"name\": \"Cypher Negative Dataset\",\n",
      "    \"description\": null,\n",
      "    \"created_at\": \"2025-07-11T13:46:24.846Z\",\n",
      "    \"updated_at\": \"2025-07-11T13:46:24.846Z\",\n",
      "    \"item_count\": 1,\n",
      "    \"expected_items\": \"unknown\",\n",
      "    \"exported_at\": \"2025-07-11T13:46:24.846Z\"\n",
      "  },\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"id\": \"41ae71ea-59db-46b7-af56-d8f5026ead56\",\n",
      "      \"status\": \"ACTIVE\",\n",
      "      \"input\": [\n",
      "        {\n",
      "          \"role\": \"system\",\n",
      "          \"context\": \"\\n<objective>\\nYou are Cypher Query BotAI for t...\n"
     ]
    }
   ],
   "source": [
    "# Load and preview the log data\n",
    "with open('logs.json', 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "\n",
    "# Check the type and structure of the loaded data\n",
    "print(f\"Type of log_data: {type(log_data)}\")\n",
    "\n",
    "if isinstance(log_data, list):\n",
    "    print(f\"Number of log entries: {len(log_data)}\")\n",
    "    if log_data:\n",
    "        print(\"\\nFirst log entry structure:\")\n",
    "        print(json.dumps(log_data[0], indent=2)[:500] + \"...\")\n",
    "elif isinstance(log_data, dict):\n",
    "    print(f\"Keys in log_data: {list(log_data.keys())}\")\n",
    "    if 'logs' in log_data:\n",
    "        print(f\"Number of log entries: {len(log_data['logs'])}\")\n",
    "        print(\"\\nFirst log entry structure:\")\n",
    "        print(json.dumps(log_data['logs'][0], indent=2)[:500] + \"...\")\n",
    "    else:\n",
    "        # Show the structure if 'logs' key doesn't exist\n",
    "        print(\"\\nLog data structure:\")\n",
    "        print(json.dumps(log_data, indent=2)[:500] + \"...\")\n",
    "else:\n",
    "    print(f\"Unexpected data type: {type(log_data)}\")\n",
    "    print(str(log_data)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAGAs evaluator initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize the evaluator\n",
    "evaluator = RAGAsEvaluator()\n",
    "print(\"‚úÖ RAGAs evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 1/10: 41ae71ea-59db-46b7-af56-d8f5026ead56\n",
      "  Faithfulness: 0.0, Relevancy: 0.848, Precision: 1.0\n",
      "Evaluating 2/10: f2530571-175f-479e-a21b-2e096d186cd8\n",
      "  Faithfulness: 0.0, Relevancy: 0.839, Precision: 1.0\n",
      "Evaluating 3/10: 77e16750-3981-4751-a862-ea21bbbd2a1d\n",
      "  Faithfulness: 0.8, Relevancy: 0.0, Precision: 1.0\n",
      "Evaluating 4/10: 80e8a1e6-6ba2-4b38-9397-56b89564ca00\n",
      "  Faithfulness: 1.0, Relevancy: 0.8416, Precision: 1.0\n",
      "Evaluating 5/10: 3a1d050a-502c-4af7-8ed3-9dd55f13b06a\n",
      "  Faithfulness: 0.2308, Relevancy: 0.8608, Precision: 1.0\n",
      "Evaluating 6/10: da3e35f3-da5f-4453-9d68-5c2f951b2d85\n",
      "Faithfulness error: Failed to parse NLIStatementOutput from completion {\"statements\": [{\"statement\": \"The first code block matches a brand node to a car node.\", \"reason\": \"The provided context describes a Cypher Query BotAI designed to transform car-related questions into Cypher queries. It outlines the schema, mandates, strategies, and rules for generating these queries. However, it does not contain any specific code blocks or queries to analyze. Therefore, it's impossible to determine if the first code block matches a brand node to a car node.\", \"verdict\": 0}, {\"statement\": \"The relationship between the brand node and the car node is has_model.\", \"reason\": \"The provided context describes a Cypher Query BotAI designed to transform car-related questions into Cypher queries. It outlines the schema, mandates, strategies, and rules for generating these queries. The knowledge graph schema section specifies that the relationship between brand and car is (:brand)-[:has_model]->(:car).\", \"verdict\": 1}, {\"statement\": \"The brand node's name property should contain \\\"gac motor\\\".\", \"reason\": \"The provided context describes a Cypher Query BotAI designed to transform car-related questions into Cypher queries. It outlines the schema, mandates, strategies, and rules for generating these queries. However, without a specific query, it's impossible to determine if the brand node's name property should contain \\\"gac motor\\\". The validation entities section lists 'gac motor' as a brand.\", \"verdict\": 0}, {\"statement\": \"The car node's name property should contain \\\"emkoo\\\".\", \"reason\": \"The provided context describes a Cypher Query BotAI designed to transform car-related questions into Cypher queries. It outlines the schema, mandates, strategies, and rules for generating these queries. However, without a specific query, it's impossible to determine if the car node's name property should contain \\\"emkoo\\\". The validation entities section lists 'emkoo' as a model of 'gac motor'.\", \"verdict\": 0}, {\"statement\": \"The first code block returns the car node's name as car_name.\", \"reason\": \"The provided context describes a Cypher Query BotAI designed to transform car-related questions into Cypher queries. It outlines the schema, mandates, strategies, and rules for generating these queries. However, it does not contain any specific code blocks or queries to analyze. Therefore, it's impossible to determine if the first code block returns the car node's name as car_name.\", \"verdict\": 0}, {\"statement\": \"The first code block returns the car node's interior_colors as available_interior_colors.\", \"reason\": \"The provided context describes a Cypher Query BotAI designed to transform car-related questions into Cypher queries. It outlines the schema, mandates, strategies, and rules for generating these queries. However, it does not contain any specific code blocks or queries to analyze. Therefore, it's impossible to determine if the first code block returns the car node's interior_colors as available_interior_colors.\", \"verdict\": 0}, {\"statement\": \"The first code block returns the car node's exterior_colors as available_exterior_colors.\", \"reason\": \"The provided context describes a Cypher Query BotAI designed to transform car-related questions into Cypher queries. It outlines the schema, mandates, strategies, and rules for generating these queries. However, it does not contain any specific code blocks or queries to analyze. Therefore, it's impossible to determine if the first code block returns the car node's exterior_colors as available_exterior_colors.\", \"verdict\": 0}, {\"statement\": \"The first code block limits the result to 1.\", \"reason\": \"The provided context describes a Cypher Query BotAI designed to transform car-related questions into Cypher queries. It outlines the schema, mandates, strategies, and rules for generating these queries. However, it does not contain any specific code blocks or queries to analyze. Therefore, it's impossible to determine if the first code block limits the result to 1.\", \"verdict\": 0}, {\"statement\": \"The second code block\"}]}. Got: 2 validation errors for NLIStatementOutput\n",
      "statements.8.reason\n",
      "  Field required [type=missing, input_value={'statement': 'The second code block'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "statements.8.verdict\n",
      "  Field required [type=missing, input_value={'statement': 'The second code block'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "  Faithfulness: 0.0, Relevancy: 0.811, Precision: 1.0\n",
      "Evaluating 7/10: ba7cb5ba-b87a-491f-a4a7-6ce72b26afd2\n",
      "  Faithfulness: 0.0, Relevancy: 0.7707, Precision: 1.0\n",
      "Evaluating 8/10: 710acf9d-867a-43f4-9873-70ca08989d79\n",
      "  Faithfulness: 1.0, Relevancy: 0.7512, Precision: 1.0\n",
      "Evaluating 9/10: f7097108-3b58-415b-b3ce-31db680e1701\n",
      "Faithfulness error: Failed to parse NLIStatementOutput from completion {\"statements\": [{\"statement\": \"The provided text includes a Cypher query.\", \"reason\": \"The provided text describes the task of generating Cypher queries and includes multiple examples of Cypher queries.\", \"verdict\": 1}, {\"statement\": \"The Cypher query matches a brand node to a car node through a has_model relationship.\", \"reason\": \"The knowledge graph schema defines the relationship (:brand)-[:has_model]->(:car).\", \"verdict\": 1}, {\"statement\": \"The Cypher query matches the car node to a variant node through a has_variant relationship.\", \"reason\": \"The knowledge graph schema defines the relationship (:car)-[:has_variant]->(:variant).\", \"verdict\": 1}, {\"statement\": \"The Cypher query matches the variant node to a feature node through a has_standard_feature relationship.\", \"reason\": \"The knowledge graph schema defines the relationship (:variant)-[:has_standard_feature]->(:feature).\", \"verdict\": 1}, {\"statement\": \"The brand node name property must contain \\\"gac motor\\\".\", \"reason\": \"The validation entities list 'gac motor' as a brand.\", \"verdict\": 1}, {\"statement\": \"The car node name property must contain \\\"gs3 emzoom\\\".\", \"reason\": \"The validation entities list 'gs3 emzoom' as a model of 'gac motor'.\", \"verdict\": 1}, {\"statement\": \"The feature node name property must contain \\\"seat material\\\".\", \"reason\": \"The validation entities do not list \\\"seat material\\\" as a spec_type for gs3 emzoom or any other model. The instructions also state to avoid matching entire user phrases directly.\", \"verdict\": 0}, {\"statement\": \"The Cypher query returns the distinct feature name as feature_name.\", \"reason\": \"The example queries show the use of \\\"AS\\\" to rename the returned properties.\", \"verdict\": 1}, {\"statement\": \"The Cypher query returns the distinct feature description as feature_description.\", \"reason\": \"The example queries do not show returning feature descriptions.\", \"verdict\": 0}, {\"statement\": \"The Cypher query limits the results to 1.\", \"reason\": \"The generation rules mention LIMIT 10-20 for primary entities in broad searches and sampling limits of 10 for features/specs. There is no mention of limiting to 1.\", \"verdict\": 0}, {\"statement\": \"The provided text includes another Cypher query.\", \"reason\": \"The output requirements state that exactly two Cypher queries must be generated.\", \"verdict\": 1}, {\"statement\": \"The Cypher query matches a brand node to a car node through a has_model relationship.\", \"reason\": \"The knowledge graph schema defines the relationship (:brand)-[:has_model]->(:car).\", \"verdict\": 1}, {\"statement\": \"The Cypher query matches the car node to a specification node through a has_specs relationship.\", \"reason\": \"The knowledge graph schema defines the relationship (:car)-[:has_specs]->(:specification).\", \"verdict\": 1}, {\"statement\": \"The brand node name property must contain \\\"gac motor\\\".\", \"reason\": \"The validation entities list 'gac motor' as a brand.\", \"verdict\": 1}, {\"statement\": \"The car node name property must contain \\\"gs3 emzoom\\\".\", \"reason\": \"The validation entities list 'gs3 emzoom' as a model of 'gac motor'.\", \"verdict\": 1}, {\"statement\": \"The specification node spec_type property must contain \\\"seat material\\\".\", \"reason\": \"The validation entities do not list \\\"seat material\\\" as a spec_type for gs3 emzoom or any other model. The instructions also state to avoid matching entire user phrases directly.\", \"verdict\": 0}, {\"statement\": \"The Cypher query returns the distinct specification type as specification_type.\", \"reason\": \"The example queries show the use of \\\"AS\\\" to rename the returned properties.\", \"verdict\": 1}, {\"statement\": \"The Cypher query returns the distinct specification value as specification_value.\", \"reason\": \"The example queries show the use of \\\"AS\\\" to rename the returned properties.\", \"verdict\": 1}, {\"statement\": \"The Cypher query limits the results to 1.\", \"reason\": \"The generation rules mention LIMIT 10-20 for primary entities in broad searches and sampling limits of 10 for features/\"}]}. Got: 1 validation error for NLIStatementOutput\n",
      "statements.18.verdict\n",
      "  Field required [type=missing, input_value={'statement': 'The Cypher...ts of 10 for features/'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "  Faithfulness: 0.0, Relevancy: 0.8785, Precision: 1.0\n",
      "Evaluating 10/10: 9bd1b731-bd4d-4da0-85b5-33266fbc5245\n",
      "  Faithfulness: 0.0, Relevancy: 0.8298, Precision: 1.0\n",
      "\n",
      "=== Evaluation Complete ===\n",
      "Total samples evaluated: 10\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation with sample data (not the empty logs.json)\n",
    "import asyncio\n",
    "\n",
    "async def run_evaluation():\n",
    "    results = await evaluator.evaluate_logs(\"sample_logs.json\")  # Use sample_logs.json instead\n",
    "    return results\n",
    "\n",
    "# Run the evaluation\n",
    "results = await run_evaluation()\n",
    "\n",
    "print(f\"\\n=== Evaluation Complete ===\")\n",
    "print(f\"Total samples evaluated: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Detailed Analysis ===\n",
      "Total samples evaluated: 10\n",
      "\n",
      "üìä Score Statistics:\n",
      "Faithfulness - Mean: 0.303, Std: 0.421\n",
      "Answer Relevancy - Mean: 0.743, Std: 0.250\n",
      "Context Precision - Mean: 1.000, Std: 0.000\n",
      "\n",
      "‚ö†Ô∏è Samples with issues:\n",
      "ID 41ae71ea-59db-46b7-af56-d8f5026ead56: Low/Zero Faithfulness\n",
      "ID f2530571-175f-479e-a21b-2e096d186cd8: Low/Zero Faithfulness\n",
      "ID 77e16750-3981-4751-a862-ea21bbbd2a1d: Low Relevancy\n",
      "ID da3e35f3-da5f-4453-9d68-5c2f951b2d85: Low/Zero Faithfulness\n",
      "ID ba7cb5ba-b87a-491f-a4a7-6ce72b26afd2: Low/Zero Faithfulness\n",
      "ID f7097108-3b58-415b-b3ce-31db680e1701: Low/Zero Faithfulness\n",
      "ID 9bd1b731-bd4d-4da0-85b5-33266fbc5245: Low/Zero Faithfulness\n",
      "\n",
      "‚úÖ Top performing samples:\n",
      "ID 80e8a1e6-6ba2-4b38-9397-56b89564ca00: Total=2.842 (F:1.0, R:0.8416, P:1.0)\n",
      "ID 710acf9d-867a-43f4-9873-70ca08989d79: Total=2.751 (F:1.0, R:0.7512, P:1.0)\n",
      "ID 3a1d050a-502c-4af7-8ed3-9dd55f13b06a: Total=2.092 (F:0.2308, R:0.8608, P:1.0)\n"
     ]
    }
   ],
   "source": [
    "# Analyze the evaluation results\n",
    "print(\"=== Detailed Analysis ===\")\n",
    "print(f\"Total samples evaluated: {len(results)}\")\n",
    "\n",
    "if results:\n",
    "    # Calculate statistics\n",
    "    faithfulness_scores = [r[\"faithfulness\"] for r in results]\n",
    "    relevancy_scores = [r[\"answer_relevancy\"] for r in results]\n",
    "    precision_scores = [r[\"context_precision\"] for r in results]\n",
    "    \n",
    "    print(f\"\\nüìä Score Statistics:\")\n",
    "    print(f\"Faithfulness - Mean: {np.mean(faithfulness_scores):.3f}, Std: {np.std(faithfulness_scores):.3f}\")\n",
    "    print(f\"Answer Relevancy - Mean: {np.mean(relevancy_scores):.3f}, Std: {np.std(relevancy_scores):.3f}\")\n",
    "    print(f\"Context Precision - Mean: {np.mean(precision_scores):.3f}, Std: {np.std(precision_scores):.3f}\")\n",
    "    \n",
    "    # Identify problematic samples\n",
    "    print(f\"\\n‚ö†Ô∏è Samples with issues:\")\n",
    "    for result in results:\n",
    "        issues = []\n",
    "        if result[\"faithfulness\"] == 0.0:\n",
    "            issues.append(\"Low/Zero Faithfulness\")\n",
    "        if result[\"answer_relevancy\"] < 0.7:\n",
    "            issues.append(\"Low Relevancy\")\n",
    "        if result[\"context_precision\"] < 0.8:\n",
    "            issues.append(\"Low Precision\")\n",
    "        \n",
    "        if issues:\n",
    "            print(f\"ID {result['id']}: {', '.join(issues)}\")\n",
    "    \n",
    "    # Show top performing samples\n",
    "    print(f\"\\n‚úÖ Top performing samples:\")\n",
    "    sorted_results = sorted(results, key=lambda x: x[\"faithfulness\"] + x[\"answer_relevancy\"] + x[\"context_precision\"], reverse=True)\n",
    "    for result in sorted_results[:3]:\n",
    "        total_score = result[\"faithfulness\"] + result[\"answer_relevancy\"] + result[\"context_precision\"]\n",
    "        print(f\"ID {result['id']}: Total={total_score:.3f} (F:{result['faithfulness']}, R:{result['answer_relevancy']}, P:{result['context_precision']})\")\n",
    "else:\n",
    "    print(\"No results to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ragas_evaluation_results.json\n",
      "‚úÖ Results saved:\n",
      "  - ragas_evaluation_results.json (detailed results)\n",
      "  - ragas_summary_report.json (summary report)\n",
      "\n",
      "üéØ Final Summary:\n",
      "Average Faithfulness: 0.3031\n",
      "Average Answer Relevancy: 0.7431\n",
      "Average Context Precision: 1.0\n",
      "Overall Average Score: 0.6821\n"
     ]
    }
   ],
   "source": [
    "# Save results and create summary report\n",
    "if results:\n",
    "    # Save the detailed results\n",
    "    evaluator.save_results(results, \"ragas_evaluation_results.json\")\n",
    "    \n",
    "    # Create a summary report\n",
    "    summary_report = {\n",
    "        \"evaluation_summary\": {\n",
    "            \"total_samples\": len(results),\n",
    "            \"evaluation_date\": \"2025-07-26\",\n",
    "            \"model_used\": \"gemini-2.0-flash\",\n",
    "            \"metrics_computed\": [\"faithfulness\", \"answer_relevancy\", \"context_precision\"]\n",
    "        },\n",
    "        \"average_scores\": {\n",
    "            \"faithfulness\": round(np.mean([r[\"faithfulness\"] for r in results]), 4),\n",
    "            \"answer_relevancy\": round(np.mean([r[\"answer_relevancy\"] for r in results]), 4),\n",
    "            \"context_precision\": round(np.mean([r[\"context_precision\"] for r in results]), 4)\n",
    "        },\n",
    "        \"score_distribution\": {\n",
    "            \"faithfulness\": {\n",
    "                \"min\": round(np.min([r[\"faithfulness\"] for r in results]), 4),\n",
    "                \"max\": round(np.max([r[\"faithfulness\"] for r in results]), 4),\n",
    "                \"std\": round(np.std([r[\"faithfulness\"] for r in results]), 4)\n",
    "            },\n",
    "            \"answer_relevancy\": {\n",
    "                \"min\": round(np.min([r[\"answer_relevancy\"] for r in results]), 4),\n",
    "                \"max\": round(np.max([r[\"answer_relevancy\"] for r in results]), 4),\n",
    "                \"std\": round(np.std([r[\"answer_relevancy\"] for r in results]), 4)\n",
    "            },\n",
    "            \"context_precision\": {\n",
    "                \"min\": round(np.min([r[\"context_precision\"] for r in results]), 4),\n",
    "                \"max\": round(np.max([r[\"context_precision\"] for r in results]), 4),\n",
    "                \"std\": round(np.std([r[\"context_precision\"] for r in results]), 4)\n",
    "            }\n",
    "        },\n",
    "        \"detailed_results\": results\n",
    "    }\n",
    "    \n",
    "    # Save summary report\n",
    "    with open(\"ragas_summary_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary_report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"‚úÖ Results saved:\")\n",
    "    print(\"  - ragas_evaluation_results.json (detailed results)\")\n",
    "    print(\"  - ragas_summary_report.json (summary report)\")\n",
    "    \n",
    "    # Display final summary\n",
    "    print(f\"\\nüéØ Final Summary:\")\n",
    "    print(f\"Average Faithfulness: {summary_report['average_scores']['faithfulness']}\")\n",
    "    print(f\"Average Answer Relevancy: {summary_report['average_scores']['answer_relevancy']}\")\n",
    "    print(f\"Average Context Precision: {summary_report['average_scores']['context_precision']}\")\n",
    "    \n",
    "    overall_score = (summary_report['average_scores']['faithfulness'] + \n",
    "                    summary_report['average_scores']['answer_relevancy'] + \n",
    "                    summary_report['average_scores']['context_precision']) / 3\n",
    "    print(f\"Overall Average Score: {overall_score:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No results to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
